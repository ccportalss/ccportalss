# RAG(Retrieval Augmented Generation) 시스템 교육 가이드

## 📚 RAG란 무엇인가?

**RAG (Retrieval Augmented Generation)**는 외부 지식을 검색하여 LLM의 답변을 보강하는 AI 아키텍처입니다.

### 전통적인 LLM vs RAG
- **전통적인 LLM**: 훈련 데이터에만 의존 → 환각, 구시대 정보 문제
- **RAG**: 실시간 외부 문서 검색 + LLM 생성 → 정확하고 최신의 답변

## 🏗️ RAG 아키텍처 상세 분석

### Phase 1: 문서 준비 및 벡터화
```
📄 PDF 문서
    ↓ (1) PDF.js로 텍스트 추출
📝 원시 텍스트 
    ↓ (2) 청킹(Chunking) - 의미 단위로 분할
📄 텍스트 청크들
    ↓ (3) OpenAI Embeddings API
🧮 1536차원 벡터들
    ↓ (4) Pinecone Vector DB 저장
🗄️ 검색 가능한 벡터 데이터베이스
```

### Phase 2: 질문-답변 생성
```
❓ 사용자 질문
    ↓ (1) OpenAI Embeddings API
🧮 질문 벡터 (1536차원)
    ↓ (2) Pinecone 코사인 유사도 검색
📄 관련 문서 청크들 (Top-K)
    ↓ (3) 컨텍스트 + 질문 결합
🤖 GPT-3.5-turbo 프롬프트
    ↓ (4) 스트리밍 답변 생성
💬 컨텍스트 기반 정확한 답변
```

## 🔬 각 단계 상세 설명

### 1. PDF 텍스트 추출
**목적**: PDF 문서에서 기계가 읽을 수 있는 텍스트를 추출
**기술**: PDF.js - 브라우저에서 실행되는 JavaScript 라이브러리
**과정**:
- PDF 파일을 바이트 배열로 읽기
- 각 페이지별로 텍스트 객체 추출
- 위치 정보와 함께 순차적으로 텍스트 결합
- 페이지 번호와 함께 전체 텍스트 구성

### 2. 텍스트 청킹 (Chunking)
**목적**: 긴 문서를 검색하기 적합한 의미 단위로 분할
**전략**: 
- 청크 크기: 1000자 (토큰 제한 고려)
- 분할 단위: 문장 경계 우선
- 오버랩: 문맥 손실 방지를 위한 중복 구간
**중요성**: 
- 너무 크면: 검색 정밀도 하락
- 너무 작으면: 문맥 정보 손실

### 3. 벡터 임베딩 생성
**모델**: OpenAI text-embedding-3-small
**차원**: 1536차원 벡터
**의미**: 각 차원이 텍스트의 의미적 특성을 수치로 표현
**예시**: 
- "강아지" → [0.23, -0.15, 0.78, ...]
- "개" → [0.21, -0.13, 0.76, ...] (유사한 벡터)

### 4. 벡터 데이터베이스 저장
**DB**: Pinecone (관리형 벡터 데이터베이스)
**인덱스**: 고차원 벡터의 효율적 검색을 위한 자료구조
**메트릭**: 코사인 유사도 (텍스트에 최적화)
**메타데이터**: 페이지 번호, 청크 ID, 원본 텍스트 등

### 5. 유사도 검색
**쿼리**: 사용자 질문을 동일한 방식으로 벡터화
**검색**: 코사인 유사도로 가장 관련성 높은 K개 청크 탐색
**순위**: 유사도 점수(0~1)로 관련성 정도 측정
**임계값**: 최소 유사도 점수로 노이즈 필터링

### 6. 컨텍스트 증강 생성
**컨텍스트**: 검색된 청크들을 하나의 문맥으로 결합
**프롬프트**: 시스템 프롬프트 + 컨텍스트 + 사용자 질문
**생성**: GPT-3.5-turbo로 컨텍스트 기반 답변 생성
**스트리밍**: 실시간으로 토큰 단위 답변 스트리밍

## 🎯 핵심 개념들

### Vector Similarity (벡터 유사도)
```
질문: "AI의 환각 현상은 무엇인가?"
벡터: [0.1, 0.8, -0.3, 0.5, ...]

문서1: "LLM의 hallucination 문제..."  
벡터: [0.2, 0.7, -0.2, 0.4, ...]
유사도: 0.89 (높음)

문서2: "날씨가 좋습니다..."
벡터: [-0.5, 0.1, 0.8, -0.2, ...]  
유사도: 0.12 (낮음)
```

### Semantic Search vs Keyword Search
**키워드 검색**: "AI 환각" → "AI", "환각" 단어 매칭
**의미적 검색**: "AI 환각" → "hallucination", "거짓 정보", "생성 오류" 등도 찾음

### Context Window Management
**문제**: LLM의 컨텍스트 길이 제한 (4K~32K 토큰)
**해결**: 가장 관련성 높은 청크들만 선별하여 사용

## ⚡ 성능 최적화

### 청킹 전략
- **고정 길이**: 간단하지만 문맥 절단 위험
- **문장 단위**: 의미 보존, 가변 길이
- **의미적 분할**: AI로 의미 경계 판단 (고급)

### 검색 최적화  
- **하이브리드 검색**: 키워드 + 벡터 검색 결합
- **리랭킹**: 초기 검색 후 더 정교한 재순위화
- **메타데이터 필터링**: 날짜, 카테고리 등으로 사전 필터링

### 답변 품질 향상
- **Chain of Thought**: 단계별 추론 과정
- **Self-consistency**: 여러 번 생성 후 일관성 체크
- **Fact-checking**: 외부 소스로 사실 검증

## 🚨 주의사항 및 한계

### 1. 환각(Hallucination) 여전히 존재
- 컨텍스트에 없는 정보를 지어낼 수 있음
- 검색된 문서와 모순되는 답변 가능

### 2. 검색 품질에 의존
- 부정확한 청킹 → 잘못된 컨텍스트
- 임베딩 모델 한계 → 의미 이해 오류

### 3. 지연 시간 (Latency)
- 벡터 검색 + LLM 생성 = 추가 지연
- 실시간 응용에는 최적화 필요

### 4. 비용
- 임베딩 생성 비용
- 벡터 DB 저장 비용  
- LLM API 호출 비용

## 📊 성능 측정 지표

### 검색 성능
- **Recall@K**: 관련 문서 중 검색된 비율
- **Precision@K**: 검색된 문서 중 관련 문서 비율
- **MRR**: 첫 번째 관련 문서의 평균 순위

### 생성 성능  
- **Faithfulness**: 컨텍스트에 충실한 정도
- **Answer Relevancy**: 질문과의 관련성
- **Context Precision**: 사용된 컨텍스트의 정확성

## 🔮 향후 발전 방향

### 1. 멀티모달 RAG
- 텍스트 + 이미지 + 테이블 통합 검색
- 시각적 정보까지 활용한 답변

### 2. 실시간 업데이트
- 문서 변경 시 자동 벡터 재생성
- 증분 업데이트로 효율성 향상

### 3. 개인화
- 사용자별 맞춤 검색
- 질문 이력 기반 컨텍스트 개선

### 4. 자동 평가
- 답변 품질 자동 모니터링
- 피드백 루프 구축
import { useState } from 'react';
import { OpenAIService } from '../services/openai';
import { PineconeClient } from '../services/pineconeClient';
import { Message, VectorSearchResult, APIConfig } from '../types';

export const useRAGChat = (indexName: string) => {
  const [messages, setMessages] = useState<Message[]>([]);
  const [isGenerating, setIsGenerating] = useState(false);
  const [searchResults, setSearchResults] = useState<VectorSearchResult[]>([]);
  const [searchStep, setSearchStep] = useState<{
    status: 'idle' | 'embedding' | 'searching' | 'generating' | 'completed';
    details: string;
  }>({ status: 'idle', details: '' });

  const sendMessage = async (content: string) => {
    // Load API config from localStorage
    const apiConfigStr = localStorage.getItem('apiConfig');
    if (!apiConfigStr) {
      throw new Error('API ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤');
    }
    
    const apiConfig: APIConfig = JSON.parse(apiConfigStr);
    
    // Initialize services
    const openaiService = new OpenAIService(apiConfig.openaiApiKey);
    const pineconeClient = new PineconeClient(
      apiConfig.pineconeApiKey,
      indexName,
      apiConfig.pineconeEnvironment
    );

    // Add user message
    const userMessage: Message = {
      content,
      type: 'user',
      timestamp: new Date()
    };
    
    setMessages(prev => [...prev, userMessage]);
    setIsGenerating(true);
    setSearchResults([]);

    let botMessage: Message | null = null;
    
    try {
      // Step 1: Create embedding for the query  
      setSearchStep({ 
        status: 'embedding', 
        details: 'ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ 1536ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì¤‘...' 
      });
      
      // Educational delay
      await new Promise(resolve => setTimeout(resolve, 1200));
      
      const queryEmbedding = await openaiService.createEmbedding(content);
      
      await new Promise(resolve => setTimeout(resolve, 500));

      // Step 2: Search similar vectors
      setSearchStep({ 
        status: 'searching', 
        details: 'ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì§ˆë¬¸ ë²¡í„°ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì²­í¬ë“¤ì„ ì°¾ëŠ” ì¤‘...' 
      });
      
      await new Promise(resolve => setTimeout(resolve, 1500));
      
      const searchResults = await pineconeClient.query(queryEmbedding, 10, 0.0); // ì„ê³„ê°’ 0ìœ¼ë¡œ ì„¤ì •
      setSearchResults(searchResults);
      
      await new Promise(resolve => setTimeout(resolve, 800));
      
      console.log('ğŸ” ê²€ìƒ‰ ê²°ê³¼:', {
        query: content,
        resultsCount: searchResults.length,
        results: searchResults.map(r => ({
          score: r.score,
          content: r.content.substring(0, 100) + '...'
        }))
      });

      if (searchResults.length === 0) {
        setSearchStep({ status: 'completed', details: 'ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤' });
        
        botMessage = {
          content: 'ì£„ì†¡í•©ë‹ˆë‹¤. ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.',
          type: 'bot',
          timestamp: new Date()
        };
        
        setMessages(prev => [...prev, botMessage!]);
        return;
      }

      // Step 3: Generate response with context
      setSearchStep({ 
        status: 'generating', 
        details: `${searchResults.length}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì´ì œ ì—°ê²°ëœ AI ëª¨ë¸ì´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤...` 
      });
      
      await new Promise(resolve => setTimeout(resolve, 1000));

      const context = searchResults
        .map(result => `[í˜ì´ì§€ ${result.metadata.pageNumber}] ${result.content}`)
        .join('\n\n');

      // Create bot message placeholder for streaming
      botMessage = {
        content: '',
        type: 'bot',
        timestamp: new Date()
      };
      
      setMessages(prev => [...prev, botMessage!]);

      // Generate response with streaming
      await openaiService.generateResponse(
        content,
        context,
        (chunk: string) => {
          // Update the bot message content as chunks arrive
          setMessages(prev => prev.map((msg, index) => 
            index === prev.length - 1 && msg.type === 'bot' 
              ? { ...msg, content: msg.content + chunk }
              : msg
          ));
        }
      );

      setSearchStep({ status: 'completed', details: 'ë‹µë³€ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤' });

    } catch (error) {
      console.error('RAG chat error:', error);
      
      const errorMessage: Message = {
        content: `ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ${error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`,
        type: 'bot',
        timestamp: new Date()
      };

      // Remove the placeholder message if it exists and add error message
      if (botMessage && botMessage.content === '') {
        setMessages(prev => [...prev.slice(0, -1), errorMessage]);
      } else {
        setMessages(prev => [...prev, errorMessage]);
      }
      
      setSearchStep({ status: 'idle', details: '' });
    } finally {
      setIsGenerating(false);
    }
  };

  return {
    messages,
    sendMessage,
    isGenerating,
    searchResults,
    searchStep
  };
};